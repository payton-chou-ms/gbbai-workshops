{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Orchestration with a Planner\n",
    "\n",
    "In this workshop, we will use the planner to design a multi-agent **FLEET** designated for the task of generating a cybersecurity report. The following agents will be included:\n",
    "\n",
    "1. time_keeper\n",
    "2. cyber_collector\n",
    "3. db_reader\n",
    "4. data_analyzer\n",
    "5. security_evaluator\n",
    "6. report_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "logging.getLogger('azure.core.pipeline.policies.http_logging_policy').setLevel(\n",
    "    logging.WARNING)\n",
    "\n",
    "load_dotenv()\n",
    "print(os.getenv('CHAT_MODEL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(), conn_str=os.environ[\"AIPROJECT_CONNECTION_STRING\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'asst_UJjhDpUdM5rbSpNaBcWD28La',\n",
       "  'name': 'report_writer',\n",
       "  'description': 'An advanced AI agent responsible for integrating information and analysis results from various agents to produce a professional, detailed, and actionable cybersecurity report.'},\n",
       " {'id': 'asst_yFHWy79liAvfwsdGKQTcGqAp',\n",
       "  'name': 'security_evaluator',\n",
       "  'description': 'AI agent that assists users in generating radar charts for cybersecurity metrics.'},\n",
       " {'id': 'asst_TZ8XzqpP0rld787V80Nv3Yhb',\n",
       "  'name': 'data_analyzer',\n",
       "  'description': 'An agent that analyzes data using forecasting or anomaly detection.'},\n",
       " {'id': 'asst_OADxbrRxRnY3PgylKysvgp34',\n",
       "  'name': 'db_reader',\n",
       "  'description': 'This agent fetches data from a specified SQLite database table, saves it as a CSV file, and returns the status and file path in JSON format.'},\n",
       " {'id': 'asst_bn1DO0L5aTqw604kfLerrZgU',\n",
       "  'name': 'cyber_collector',\n",
       "  'description': 'The cyber collector can collect information about cyber security from its knowledge base.'},\n",
       " {'id': 'asst_sLHfem370yrvlFUOJ0ojk3g2',\n",
       "  'name': 'time_keeper',\n",
       "  'description': 'An AI agent specialized in providing accurate and real-time time information.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fleet_name = \"internet_threat_analysis\"\n",
    "agent_fleet = []\n",
    "agent_list = project_client.agents.list_agents().data\n",
    "for _agent in agent_list:\n",
    "    if \"group\" in _agent.metadata.keys() and _agent.metadata[\"group\"] == fleet_name:\n",
    "        agent_fleet.append({\"id\": _agent.id, \n",
    "                            \"name\": _agent.name,\n",
    "                            \"description\": _agent.description})\n",
    "        \n",
    "agent_fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_client = project_client.inference.get_azure_openai_client(\n",
    "    api_version=\"2024-06-01\")\n",
    "\n",
    "deployment_name = os.environ[\"CHAT_MODEL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "\n",
    "您是一個智能助手，能夠回應用戶的查詢。當您收到用戶的消息時，您需要判斷是否可以直接回答，或者是否更適合分配其中一位列出的代理來回應。\n",
    "\n",
    "[__TERMINATION__] 如果您認為用戶的問題已經完全回答，請以[__TERMINATION__]作為前綴，然後回應您的回答。您的回答應提供所有相關信息和下載鏈接，以符合用戶的查詢，並附上結尾信息。不要僅僅總結對話，而是提供一個清晰簡潔的答案，包括相關鏈接。\n",
    "\n",
    "直接回應：如果問題在您的知識和能力範圍內，您將直接回應。您的回應應以前綴 [__PLANNER__] 開頭，然後是您的答案。\n",
    "\n",
    "代理分配：如果查詢更適合由某位代理處理，您將根據用戶的消息分配最合適的代理。您的回應將以前綴 [__AGENT__] 開頭，然後是代理的名字。如果所分配的代理無法提供滿意的答案，您可以決定分配另一位可能更好地滿足用戶需求的代理。\n",
    "這是您將要合作的代理列表：\n",
    "\n",
    "[__代理列表佔位符__]\n",
    "\n",
    "在決定分配哪個代理時，請務必考慮每個代理的描述。如果某個代理無法找到足夠的信息或提供相關的回應，您可以分配另一個代理，以確保用戶獲得最相關和最有見地的答案。您的目標是通過您自己的知識或通過正確的代理，向用戶提供清晰和有幫助的信息。\n",
    "## 限制條件\n",
    "- 如果您可以直接回答問題，則不要分配代理。\n",
    "- 如果有代理人能夠回答問題，請分配該代理人。\n",
    "- 如果代理人無法找到足夠的信息或提供相關的回應，您可以分配另一位代理人，以確保用戶獲得最相關和最有見地的答案。\n",
    "- 如果您不分配代理人，您必須以前綴 [__PLANNER__] 開頭，然後是您的回應。例如，[__PLANNER__]您好，我在這裡協助您。\n",
    "- 如果您分配了代理人，您必須以前綴 [__AGENT__] 開頭，然後是代理人的名字，接著是 [__TASK__]，然後是對該代理人的請求。例如：[__AGENT__]人力資源幫助台；[__TASK__]獲取公司福利政策。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"You are an intelligent assistant capable of responding to user queries. When you receive a message from the user, you need to determine whether you can answer it directly or if it would be more appropriate to allocate one of the listed agents to respond.\n",
    "\n",
    "Direct Response: If the question falls within your knowledge and capabilities, you will respond directly. Your response should start with the prefix [__PLANNER__], followed by your answer.\n",
    "\n",
    "Agent Allocation: If the query is better suited for one of the agents, you will allocate the most appropriate agent based on the user’s message. Your response will begin with the prefix [__AGENT__], followed by the agent's name. If the allocated agent is unable to provide a satisfactory answer, you may decide to allocate a different agent that may better address the user's needs.\n",
    "\n",
    "Here is the list of agents you will be working with:\n",
    "[__AGENT_LIST_PLACEHOLDER__]\n",
    "\n",
    "Make sure to consider the descriptions of each agent when deciding which one to allocate. If an agent cannot find sufficient information or provide a relevant response, you may allocate another agent to ensure the user receives the most relevant and informed answer. Your goal is to provide clear and helpful information to the user, whether through your own knowledge or via the correct agent.\n",
    "\n",
    "## Constraints\n",
    "- Do not allocate an agent if you can answer the question directly.\n",
    "- If there is an agent that can possibly answer the question, allocate that agent.\n",
    "- If an agent cannot find sufficient information or provide a relevant response, you may allocate another agent to ensure the user receives the most relevant and informed answer.\n",
    "- If you don't allocate an agent, you must respond with the prefix [__PLANNER__], followed by your response. For example, [__PLANNER__]Hello, I'm here to assist you.;\n",
    "- If you allocate an agent, you must respond with the prefix [__AGENT__], followed by the agent's name, and then [__TASK__] followed by the request to that agent. For example: [__AGENT__]HR Helpdesk;[__TASK__]Get the company benefit policies.;\n",
    "- If you think the user's question is fully answered, respond with the prefix [__TERMINATION__] followed by your response. Your response should provide all relevant information and download links matching the user's query, followed by a closing message. Don't just make a summary of the conversation, but provide a clear and concise answer to the user's query including relevant links.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an intelligent assistant capable of responding to user queries. When you receive a message from the user, you need to determine whether you can answer it directly or if it would be more appropriate to allocate one of the listed agents to respond.\n",
      "\n",
      "Direct Response: If the question falls within your knowledge and capabilities, you will respond directly. Your response should start with the prefix [__PLANNER__], followed by your answer.\n",
      "\n",
      "Agent Allocation: If the query is better suited for one of the agents, you will allocate the most appropriate agent based on the user’s message. Your response will begin with the prefix [__AGENT__], followed by the agent's name. If the allocated agent is unable to provide a satisfactory answer, you may decide to allocate a different agent that may better address the user's needs.\n",
      "\n",
      "Here is the list of agents you will be working with:\n",
      "[{{\"id\": \"asst_UJjhDpUdM5rbSpNaBcWD28La\", \"name\": \"report_writer\", \"description\": \"An advanced AI agent responsible for integrating information and analysis results from various agents to produce a professional, detailed, and actionable cybersecurity report.\"}}, {{\"id\": \"asst_yFHWy79liAvfwsdGKQTcGqAp\", \"name\": \"security_evaluator\", \"description\": \"AI agent that assists users in generating radar charts for cybersecurity metrics.\"}}, {{\"id\": \"asst_TZ8XzqpP0rld787V80Nv3Yhb\", \"name\": \"data_analyzer\", \"description\": \"An agent that analyzes data using forecasting or anomaly detection.\"}}, {{\"id\": \"asst_OADxbrRxRnY3PgylKysvgp34\", \"name\": \"db_reader\", \"description\": \"This agent fetches data from a specified SQLite database table, saves it as a CSV file, and returns the status and file path in JSON format.\"}}, {{\"id\": \"asst_bn1DO0L5aTqw604kfLerrZgU\", \"name\": \"cyber_collector\", \"description\": \"The cyber collector can collect information about cyber security from its knowledge base.\"}}, {{\"id\": \"asst_sLHfem370yrvlFUOJ0ojk3g2\", \"name\": \"time_keeper\", \"description\": \"An AI agent specialized in providing accurate and real-time time information.\"}}]\n",
      "\n",
      "Make sure to consider the descriptions of each agent when deciding which one to allocate. If an agent cannot find sufficient information or provide a relevant response, you may allocate another agent to ensure the user receives the most relevant and informed answer. Your goal is to provide clear and helpful information to the user, whether through your own knowledge or via the correct agent.\n",
      "\n",
      "## Constraints\n",
      "- Do not allocate an agent if you can answer the question directly.\n",
      "- If there is an agent that can possibly answer the question, allocate that agent.\n",
      "- If an agent cannot find sufficient information or provide a relevant response, you may allocate another agent to ensure the user receives the most relevant and informed answer.\n",
      "- If you don't allocate an agent, you must respond with the prefix [__PLANNER__], followed by your response. For example, [__PLANNER__]Hello, I'm here to assist you.;\n",
      "- If you allocate an agent, you must respond with the prefix [__AGENT__], followed by the agent's name, and then [__TASK__] followed by the request to that agent. For example: [__AGENT__]HR Helpdesk;[__TASK__]Get the company benefit policies.;\n",
      "- If you think the user's question is fully answered, respond with the prefix [__TERMINATION__] followed by your response. Your response should provide all relevant information and download links matching the user's query, followed by a closing message. Don't just make a summary of the conversation, but provide a clear and concise answer to the user's query including relevant links.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "cyber_fleet_str = json.dumps(agent_fleet).replace(\"{\", '{{').replace(\"}\", '}}')\n",
    "print(sys_prompt.replace(\"[__AGENT_LIST_PLACEHOLDER__]\", cyber_fleet_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 產生報告的準則\n",
    "1. 從計時器開始獲取當前時間。\n",
    "2. 分配 Cyber Collector 以檢索最新的威脅資訊。\n",
    "3. 使用 cyber collector 中的指標類型讓資料庫讀取器獲取資料庫數據。\n",
    "4. 使用資料庫讀取器中的 CSV 檔讓數據分析器執行分析，包括預測和異常檢測。\n",
    "5. 分配安全評估員以創建評估指標的雷達圖。\n",
    "6. 啟動報表編寫器以編譯資訊並生成 PDF 報表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "addtional_constraint = '''\n",
    "## Guidelines to generate a report\n",
    "1. Start with the time keeper to get the current time.\n",
    "2. Allocate the cyber collector to retrieve latest threat information.\n",
    "3. Use the metric types from the cyber collector to have the db reader fetch database data.\n",
    "4. Use the CSV files from the db reader to have the data analyzer perform analysis, including forecasting and anomaly detection.\n",
    "5. Allocate the security evaluator to create radar chart of evaluation metrics.\n",
    "6. Start report writer to compile information and generate PDF report.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_functions import user_functions\n",
    "from azure.ai.projects.models import FunctionTool, RequiredFunctionToolCall, SubmitToolOutputsAction, ToolOutput\n",
    "\n",
    "functions = FunctionTool(functions=user_functions)\n",
    "\n",
    "\n",
    "def agent_execution(agent_id, task, context):\n",
    "\n",
    "    thread = project_client.agents.create_thread()\n",
    "    print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "    # Create message to thread\n",
    "    message = project_client.agents.create_message(\n",
    "        thread_id=thread.id, role=\"user\", content=f'task: {task} \\n\\n context: {context}')\n",
    "\n",
    "    print(f\"Created message, ID: {message.id}\")\n",
    "\n",
    "    # Create and process assistant run in thread with tools\n",
    "    run = project_client.agents.create_run(\n",
    "        thread_id=thread.id, agent_id=agent_id)\n",
    "    print(f\"Created run, ID: {run.id}\")\n",
    "\n",
    "    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "        run = project_client.agents.get_run(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "        if run.status == \"requires_action\" and isinstance(run.required_action, SubmitToolOutputsAction):\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            if not tool_calls:\n",
    "                print(\"No tool calls provided - cancelling run\")\n",
    "                project_client.agents.cancel_run(\n",
    "                    thread_id=thread.id, run_id=run.id)\n",
    "                break\n",
    "\n",
    "            tool_outputs = []\n",
    "            for tool_call in tool_calls:\n",
    "                if isinstance(tool_call, RequiredFunctionToolCall):\n",
    "                    try:\n",
    "                        print(f\"Executing tool call: {tool_call}\")\n",
    "                        output = functions.execute(tool_call)\n",
    "                        tool_outputs.append(\n",
    "                            ToolOutput(\n",
    "                                tool_call_id=tool_call.id,\n",
    "                                output=output,\n",
    "                            )\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error executing tool_call {tool_call.id}: {e}\")\n",
    "\n",
    "            print(f\"Tool outputs: {tool_outputs}\")\n",
    "            if tool_outputs:\n",
    "                project_client.agents.submit_tool_outputs_to_run(\n",
    "                    thread_id=thread.id, run_id=run.id, tool_outputs=tool_outputs\n",
    "                )\n",
    "\n",
    "        print(f\"Current run status: {run.status}\")\n",
    "\n",
    "    print(f\"Run completed with status: {run.status}\")\n",
    "\n",
    "    if run.status == \"failed\":\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "    messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assistant_content(agent_messages):\n",
    "    contents = []\n",
    "    for message in agent_messages:\n",
    "        if message['role'] == 'assistant':\n",
    "            contents.append(message['content'][0]['text']['value'])\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt.replace(\"[__AGENT_LIST_PLACEHOLDER__]\", cyber_fleet_str) + addtional_constraint\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a cybersecurity report for Contoso. The security evaluation metrics are: Vulnerability Score(8), Detection Rate(7), Response Time(6), Threat Intelligence(9), System Uptime(8)\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated agent: time_keeper\n",
      "Agent task: Get the current time.\n",
      "Created thread, ID: thread_dfaPBWXqjwOOoORRryABgB8Q\n",
      "Created message, ID: msg_YHWU9gdwG9xWdJ4siEcAGwkU\n",
      "Created run, ID: run_QCzYehybn84ZqOwhrW7EohUX\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Executing tool call: {'id': 'call_mseCKmBJGS8Kn2F6S23Vsc5Z', 'type': 'function', 'function': {'name': 'fetch_current_datetime', 'arguments': '{}'}}\n",
      "Tool outputs: [{'tool_call_id': 'call_mseCKmBJGS8Kn2F6S23Vsc5Z', 'output': '{\"current_time\": \"2025-03-27 19:17:32\"}'}]\n",
      "Current run status: RunStatus.REQUIRES_ACTION\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.COMPLETED\n",
      "Run completed with status: RunStatus.COMPLETED\n",
      "[__AGENT__]cyber_collector;[__TASK__]Retrieve the latest threat information for cybersecurity.\n",
      "Allocated agent: cyber_collector\n",
      "Agent task: Retrieve the latest threat information for cybersecurity.\n",
      "Created thread, ID: thread_kW9WSMzxXTO3YTLh8yaq9liK\n",
      "Created message, ID: msg_2SzUDwHxhp7q3wUpShSsIbSs\n",
      "Created run, ID: run_8ze3Q6ss27gTOtLq3TMPJe66\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.FAILED\n",
      "Run completed with status: RunStatus.FAILED\n",
      "Run failed: {'code': 'server_error', 'message': 'Sorry, something went wrong.'}\n",
      "[__AGENT__]cyber_collector;[__TASK__]Collect the latest information about cybersecurity threats.\n",
      "Allocated agent: cyber_collector\n",
      "Agent task: Collect the latest information about cybersecurity threats.\n",
      "Created thread, ID: thread_RJ5tqimwgwAJxVR3AtzXZUlL\n",
      "Created message, ID: msg_uSnthJe8XiUDswrghDwrDeV2\n",
      "Created run, ID: run_oa3Id8b6nZ4bWQRiVvjdp04f\n",
      "Current run status: RunStatus.QUEUED\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.FAILED\n",
      "Run completed with status: RunStatus.FAILED\n",
      "Run failed: {'code': 'server_error', 'message': 'Sorry, something went wrong.'}\n",
      "[__AGENT__]db_reader;[__TASK__]Fetch data for the following metrics: Vulnerability Score, Detection Rate, Response Time, Threat Intelligence, System Uptime, and save it as a CSV file.\n",
      "Allocated agent: db_reader\n",
      "Agent task: Fetch data for the following metrics: Vulnerability Score, Detection Rate, Response Time, Threat Intelligence, System Uptime, and save it as a CSV file.\n",
      "Created thread, ID: thread_C4csOrbWLbwbCJqzyuIFgBrc\n",
      "Created message, ID: msg_KigERRXA9TECzDrfGPnGqeoZ\n",
      "Created run, ID: run_4fh8hsCzLnT2GmGfMjO6yKjc\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Executing tool call: {'id': 'call_WwN8dsjWs5kIBo8Su6zg1Nun', 'type': 'function', 'function': {'name': 'fetch_data', 'arguments': '{\"table_name\": \"intrusion_attempts\"}'}}\n",
      "Executing tool call: {'id': 'call_iTQxBmIWFuxFfVdqHAfJ37Fe', 'type': 'function', 'function': {'name': 'fetch_data', 'arguments': '{\"table_name\": \"incident_detection_rate\"}'}}\n",
      "Tool outputs: [{'tool_call_id': 'call_WwN8dsjWs5kIBo8Su6zg1Nun', 'output': '{\"success\": true, \"message\": \"./data/intrusion_attempts_20250327_191750.csv\"}'}, {'tool_call_id': 'call_iTQxBmIWFuxFfVdqHAfJ37Fe', 'output': '{\"success\": true, \"message\": \"./data/incident_detection_rate_20250327_191750.csv\"}'}]\n",
      "Current run status: RunStatus.REQUIRES_ACTION\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.COMPLETED\n",
      "Run completed with status: RunStatus.COMPLETED\n",
      "[__AGENT__]data_analyzer;[__TASK__]Analyze the CSV files for forecasting and anomaly detection regarding the fetched cybersecurity data.\n",
      "Allocated agent: data_analyzer\n",
      "Agent task: Analyze the CSV files for forecasting and anomaly detection regarding the fetched cybersecurity data.\n",
      "Created thread, ID: thread_ZW8KPlsrPeiZ2PGHq8JZtqFP\n",
      "Created message, ID: msg_1JMg6J24Bj9r43ldxdwLlYZm\n",
      "Created run, ID: run_Hpq6SYicuB9IUvenJXrER6I5\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Current run status: RunStatus.IN_PROGRESS\n",
      "Executing tool call: {'id': 'call_pojHfLkdMO0uqBetUicOg3Hp', 'type': 'function', 'function': {'name': 'analyze_data', 'arguments': '{\"file_path\": \"./data/intrusion_attempts_20250327_191750.csv\", \"method\": \"forecast\", \"horizon\": 5}'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Inferred freq: MS\n",
      "INFO:nixtla.nixtla_client:Attempt 1 failed...\n",
      "INFO:nixtla.nixtla_client:Attempt 2 failed...\n",
      "INFO:nixtla.nixtla_client:Attempt 3 failed...\n",
      "INFO:nixtla.nixtla_client:Attempt 4 failed...\n",
      "INFO:nixtla.nixtla_client:Attempt 5 failed...\n",
      "INFO:nixtla.nixtla_client:Attempt 6 failed...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool call: {'id': 'call_zlfUXEpvumfPcYtw6yL3WYeb', 'type': 'function', 'function': {'name': 'analyze_data', 'arguments': '{\"file_path\": \"./data/intrusion_attempts_20250327_191750.csv\", \"method\": \"anomaly_detection\"}'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Calling Anomaly Detector Endpoint...\n",
      "INFO:nixtla.nixtla_client:Attempt 1 failed...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent_id:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     agent_messages = \u001b[43magent_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     agent_contents = get_assistant_content(agent_messages.data)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m agent_contents:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36magent_execution\u001b[39m\u001b[34m(agent_id, task, context)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuting tool call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_call\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     output = \u001b[43mfunctions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     tool_outputs.append(\n\u001b[32m     41\u001b[39m         ToolOutput(\n\u001b[32m     42\u001b[39m             tool_call_id=tool_call.id,\n\u001b[32m     43\u001b[39m             output=output,\n\u001b[32m     44\u001b[39m         )\n\u001b[32m     45\u001b[39m     )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\azure\\ai\\projects\\models\\_patch.py:677\u001b[39m, in \u001b[36mFunctionTool.execute\u001b[39m\u001b[34m(self, tool_call)\u001b[39m\n\u001b[32m    674\u001b[39m function, parsed_arguments = \u001b[38;5;28mself\u001b[39m._get_func_and_args(tool_call)\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparsed_arguments\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m parsed_arguments \u001b[38;5;28;01melse\u001b[39;00m function()\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    679\u001b[39m     error_message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError executing function \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_call.function.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\MultiAgent-CyberSecurity-Lab\\02_multi_agents\\01_orchestration_planner\\user_functions.py:161\u001b[39m, in \u001b[36manalyze_data\u001b[39m\u001b[34m(file_path, method, horizon, ftSteps)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fig_name\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33manomaly_detection\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     anomalies_df = \u001b[43mnixtla_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect_anomalies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m     anomalies_df = anomalies_df.rename(columns={\n\u001b[32m    168\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTimeGEN\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTimeGPT\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    169\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTimeGEN-lo-99\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTimeGPT-lo-99\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTimeGEN-hi-99\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTimeGPT-hi-99\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    171\u001b[39m     })\n\u001b[32m    173\u001b[39m     fig = nixtla_client.plot(\n\u001b[32m    174\u001b[39m         df, anomalies_df, time_col=\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m, target_col=\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\nixtla\\nixtla_client.py:1432\u001b[39m, in \u001b[36mNixtlaClient.detect_anomalies\u001b[39m\u001b[34m(self, df, freq, id_col, time_col, target_col, level, clean_ex_first, validate_api_key, date_features, date_features_to_one_hot, model, num_partitions)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Detect anomalies in your time series using TimeGPT.\u001b[39;00m\n\u001b[32m   1375\u001b[39m \n\u001b[32m   1376\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1429\u001b[39m \u001b[33;03m    DataFrame with anomalies flagged with 1 detected by TimeGPT.\u001b[39;00m\n\u001b[32m   1430\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, pd.DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_detect_anomalies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclean_ex_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_ex_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_api_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_features_to_one_hot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_features_to_one_hot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1447\u001b[39m     dist_nixtla_client = \u001b[38;5;28mself\u001b[39m._instantiate_distributed_nixtla_client()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\nixtla\\nixtla_client.py:742\u001b[39m, in \u001b[36mvalidate_model_parameter.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    737\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supported_models:\n\u001b[32m    738\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    739\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33munsupported model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    740\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33msupported models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.supported_models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    741\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\nixtla\\nixtla_client.py:760\u001b[39m, in \u001b[36mpartition_by_uid.<locals>.wrapper\u001b[39m\u001b[34m(self, num_partitions, **kwargs)\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_partitions, **kwargs):\n\u001b[32m    759\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m num_partitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_partitions == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    761\u001b[39m     df = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    762\u001b[39m     X_df = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mX_df\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\nixtla\\nixtla_client.py:1033\u001b[39m, in \u001b[36m_NixtlaClient._detect_anomalies\u001b[39m\u001b[34m(self, df, freq, id_col, time_col, target_col, level, clean_ex_first, validate_api_key, date_features, date_features_to_one_hot, model, num_partitions)\u001b[39m\n\u001b[32m   1014\u001b[39m nixtla_client_model = _NixtlaClientModel(\n\u001b[32m   1015\u001b[39m     client=\u001b[38;5;28mself\u001b[39m.client,\n\u001b[32m   1016\u001b[39m     h=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m     max_wait_time=\u001b[38;5;28mself\u001b[39m.max_wait_time,\n\u001b[32m   1029\u001b[39m )\n\u001b[32m   1030\u001b[39m df, X_df, uids_dtype = \u001b[38;5;28mself\u001b[39m._uids_to_categorical(\n\u001b[32m   1031\u001b[39m     df=df, X_df=\u001b[38;5;28;01mNone\u001b[39;00m, id_col=id_col\n\u001b[32m   1032\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m anomalies_df = \u001b[43mnixtla_client_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect_anomalies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1034\u001b[39m anomalies_df = \u001b[38;5;28mself\u001b[39m._restore_uids(anomalies_df, dtype=uids_dtype, id_col=id_col)\n\u001b[32m   1035\u001b[39m \u001b[38;5;28mself\u001b[39m.weights_x = nixtla_client_model.weights_x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\nixtla\\nixtla_client.py:646\u001b[39m, in \u001b[36m_NixtlaClientModel.detect_anomalies\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    644\u001b[39m main_logger.info(\u001b[33m\"\u001b[39m\u001b[33mCalling Anomaly Detector Endpoint...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    645\u001b[39m y, x = \u001b[38;5;28mself\u001b[39m.dataframes_to_dict(Y_df, X_df)\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m response_timegpt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43manomaly_detection_multi_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMultiSeriesAnomaly\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclean_ex_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclean_ex_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights_x\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response_timegpt:\n\u001b[32m    662\u001b[39m     \u001b[38;5;28mself\u001b[39m.weights_x = pd.DataFrame(\n\u001b[32m    663\u001b[39m         {\n\u001b[32m    664\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m: x_cols,\n\u001b[32m    665\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m: response_timegpt[\u001b[33m\"\u001b[39m\u001b[33mweights_x\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    666\u001b[39m         }\n\u001b[32m    667\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\nixtla\\nixtla_client.py:229\u001b[39m, in \u001b[36m_NixtlaClientModel._call_api\u001b[39m\u001b[34m(self, method, request)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_api\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, request):\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m    231\u001b[39m         response = response[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:485\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[32m    484\u001b[39m     retry_state.prepare_for_next_attempt()\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chihengchou\\Downloads\\work\\work\\gbbai-workshops\\.venv\\Lib\\site-packages\\tenacity\\nap.py:31\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "llm_response = response.choices[0].message.content\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "while \"[__AGENT__]\" in llm_response:\n",
    "    agent_name = llm_response.split(\"[__AGENT__]\")[1].split(\";\")[0]\n",
    "    print(f\"Allocated agent: {agent_name}\")\n",
    "    agent_task = llm_response.split(\"[__TASK__]\")[1]\n",
    "    print(f\"Agent task: {agent_task}\")\n",
    "    # find agent id from agent fleet based on agent_name\n",
    "    agent_id = None\n",
    "    for agent in agent_fleet:\n",
    "        if agent[\"name\"] == agent_name:\n",
    "            agent_id = agent[\"id\"]\n",
    "            break\n",
    "\n",
    "    if agent_id:\n",
    "        agent_messages = agent_execution(agent_id, agent_task, json.dumps(messages[1:]))\n",
    "        agent_contents = get_assistant_content(agent_messages.data)\n",
    "        if agent_contents:\n",
    "            for agent_content in agent_contents:\n",
    "                messages.append(\n",
    "                    {\"role\": \"assistant\", \"content\": f\"[__AGENT__({agent_name})]{agent_content}\"})\n",
    "\n",
    "        response = aoai_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        llm_response = response.choices[0].message.content\n",
    "        print(llm_response)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
