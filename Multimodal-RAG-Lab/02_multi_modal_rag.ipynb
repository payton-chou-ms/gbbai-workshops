{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalog of Contents\n",
    "\n",
    "1. [Process documents and convert to Markdown](###1.-Convert-document-to-Markdown-with-images-and-tables)\n",
    "2. [Process images in documents and update MD](###2.-Extract-images-from-document-and-save-locally-as-figure_xx.png)\n",
    "3. [Inference](#3.-Inference)\n",
    "4. [Define the helper function to concatenate function info](#Define-the-helper-function-to-concatenate-function-info)\n",
    "5. [Combine all components into a single function](#Combine-all-components-into-a-single-function)\n",
    "6. [Final test](#Final-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU openai\n",
    "%pip install -qU pyyaml\n",
    "%pip install -qU pymupdf\n",
    "%pip install -qU matplotlib\n",
    "%pip install -qU python-dotenv\n",
    "%pip install -qU tenacity==8.5.0\n",
    "%pip install -qU tiktoken==0.7.0\n",
    "%pip install -qU langchain==0.3.18\n",
    "%pip install -qU azure-core==1.31.0\n",
    "%pip install -qU langchain-core==0.3.34\n",
    "%pip install -qU azure-storage-blob==12.19.0\n",
    "%pip install -qU azure-search-documents==11.4.0\n",
    "%pip install -qU azure-ai-documentintelligence==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 1: Knowledge Ingestion\n",
    " 1. Convert document to Markdown with images and tables\n",
    " 2. Extract images from document and save locally as figure_xx.png\n",
    " 3. Loop: Generate descriptions for images and insert into original Markdown\n",
    " 4. Upload local images to cloud storage\n",
    " 5. Generate chunks, ensuring content within HTML tags remains intact\n",
    "\n",
    "Phase 2: Knowledge Retrieval\n",
    " 1. Check retrieved chunk for \"blob://\" URLs\n",
    " 2. Parse \"blob://\" URL, extract storage information, and generate SAS URL\n",
    " 3. Replace \"blob://\" with SAS URL\n",
    " 4. Generate answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.ai.documentintelligence\n",
    "\n",
    "print(azure.ai.documentintelligence.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convert document to Markdown with images and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentContentFormat, AnalyzeResult\n",
    "\n",
    "\n",
    "doc_intelligence_endpoint = os.getenv(\n",
    "    \"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=doc_intelligence_endpoint,\n",
    "    credential=AzureKeyCredential(doc_intelligence_key),\n",
    "    api_version='2024-11-30')\n",
    "\n",
    "\n",
    "def analyze_document(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\",\n",
    "            body=f,\n",
    "            content_type=\"application/octet-stream\",\n",
    "            polling_interval=3,\n",
    "            output_content_format=DocumentContentFormat.MARKDOWN\n",
    "        )\n",
    "\n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "result = analyze_document(\"./data/Overview_of_LLMs_short.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def find_page_breaks(input_html: str):\n",
    "    pattern = re.compile(r\"<!-- PageBreak -->\", re.DOTALL | re.IGNORECASE)\n",
    "    return [(m.start(), m.end()) for m in pattern.finditer(input_html)]\n",
    "\n",
    "\n",
    "def find_figure_positions(input_html: str):\n",
    "    pattern = re.compile(r\"<figure>.*?</figure>\", re.DOTALL | re.IGNORECASE)\n",
    "    return [(m.start(), m.end()) for m in pattern.finditer(input_html)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = find_figure_positions(result.content)\n",
    "print(len(figs))\n",
    "figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result.figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.figures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "markdown_content = result.content\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from process_images_v2 import include_figures_in_md\n",
    "\n",
    "input_file_path = \"./data/Overview_of_LLMs_short.pdf\"\n",
    "containerName = \"rag-test\"\n",
    "folder = \"Overview_of_LLMs_short\"\n",
    "new_md_content = await include_figures_in_md(input_file_path, result, containerName,\n",
    "                                             folder, output_folder=\"data/cropped\")\n",
    "display(Markdown(new_md_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_md_content_2 = new_md_content.replace(\n",
    "    \"blob://stkuptrxdgmh7mg/rag-test/Overview_of_LLMs_short/figure_0.png\", './data/cropped/figure_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(new_md_content_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Process retrieved chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from azure.storage.blob.aio import BlobServiceClient\n",
    "from azure.storage.blob import BlobSasPermissions, generate_blob_sas\n",
    "\n",
    "sas_token_cache = {}\n",
    "\n",
    "\n",
    "def generate_sas_url(blob_client):\n",
    "    # Generate a unique key for the blob\n",
    "    blob_key = f\"{blob_client.account_name}/{blob_client.container_name}/{blob_client.blob_name}\"\n",
    "\n",
    "    # Check if the SAS token is already cached and still valid\n",
    "    if blob_key in sas_token_cache:\n",
    "        sas_token, expiry_time = sas_token_cache[blob_key]\n",
    "        if expiry_time > datetime.now(timezone.utc):\n",
    "            # Return the cached SAS URL if the token is still valid\n",
    "            return f\"https://{blob_client.account_name}.blob.core.windows.net/{blob_client.container_name}/{blob_client.blob_name}?{sas_token}\"\n",
    "\n",
    "    # Generate a new SAS token\n",
    "    expiry_time = datetime.now(timezone.utc) + timedelta(hours=1)\n",
    "    sas_token = generate_blob_sas(\n",
    "        blob_client.account_name,\n",
    "        blob_client.container_name,\n",
    "        blob_client.blob_name,\n",
    "        account_key=blob_client.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=expiry_time\n",
    "    )\n",
    "\n",
    "    # Cache the new SAS token and its expiry time\n",
    "    sas_token_cache[blob_key] = (sas_token, expiry_time)\n",
    "\n",
    "    # Generate the SAS URL\n",
    "    sas_url = f\"https://{blob_client.account_name}.blob.core.windows.net/{blob_client.container_name}/{blob_client.blob_name}?{sas_token}\"\n",
    "\n",
    "    return sas_url\n",
    "\n",
    "\n",
    "def extract_blob_url(input_str):\n",
    "    # Define the regex pattern to match the blob URL\n",
    "    pattern = r'blob://.*?\\.png'\n",
    "\n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(pattern, input_str)\n",
    "    print(matches)\n",
    "\n",
    "    # Return the list of matched URLs\n",
    "    return matches\n",
    "\n",
    "\n",
    "def extract_elements_from_blob_url(blob_url):\n",
    "    # Split the URL by '/'\n",
    "    elements = blob_url.split('/')\n",
    "\n",
    "    # Extract the required elements\n",
    "    if len(elements) >= 6:\n",
    "        return elements[2], elements[3], elements[4], elements[5]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "async def process(md_content: str):\n",
    "    blob_urls = extract_blob_url(md_content)\n",
    "    sas_urls = {}\n",
    "\n",
    "    async with BlobServiceClient.from_connection_string(os.environ[\"AzureWebJobsStorage\"]) as storage_client:\n",
    "        for blob_url in blob_urls:\n",
    "            elements = extract_elements_from_blob_url(blob_url)\n",
    "            if elements:\n",
    "                _, container_name, blob_name = elements[0], elements[1], '/'.join(\n",
    "                    elements[2:])\n",
    "                blob_client = storage_client.get_blob_client(\n",
    "                    container=container_name, blob=blob_name)\n",
    "                sas_url = generate_sas_url(blob_client)\n",
    "                sas_urls[blob_url] = sas_url\n",
    "\n",
    "    # Replace blob URLs with SAS URLs in the markdown content\n",
    "    for blob_url, sas_url in sas_urls.items():\n",
    "        md_content = md_content.replace(blob_url, sas_url)\n",
    "\n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_urls = extract_blob_url(new_md_content)\n",
    "processed_md_content = await process(new_md_content)\n",
    "processed_md_content\n",
    "blob_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(processed_md_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Function to find complete tags and replace with placeholders\n",
    "def preserve_tags(text, tags):\n",
    "    matches = []\n",
    "    for tag in tags:\n",
    "        pattern = fr\"(<{tag}.*?>.*?</{tag}>)\"\n",
    "        found_matches = re.findall(pattern, text, re.DOTALL)\n",
    "        for i, match in enumerate(found_matches):\n",
    "            text = text.replace(match, f\"[[PLACEHOLDER_{len(matches) + i}]]\")\n",
    "        matches.extend(found_matches)\n",
    "    return text, matches\n",
    "\n",
    "\n",
    "# Function to restore tags from placeholders\n",
    "def restore_tags(text, matches):\n",
    "    for i, match in enumerate(matches):\n",
    "        text = text.replace(f\"[[PLACEHOLDER_{i}]]\", match)\n",
    "    return text\n",
    "\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\"),\n",
    "    (\"#######\", \"Header 7\"),\n",
    "    (\"########\", \"Header 8\")\n",
    "]\n",
    "\n",
    "md_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "# Process the markdown content to preserve tags\n",
    "tags_to_preserve = [\"figure\", \"table\"]\n",
    "replaced_md_content, matches = preserve_tags(new_md_content, tags_to_preserve)\n",
    "md_header_splits = md_splitter.split_text(replaced_md_content)\n",
    "\n",
    "chunk_size = 512\n",
    "chunk_overlap = 0\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split the text into documents\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "# Restore tags in each split\n",
    "restored_splits = [Document(page_content=restore_tags(split.page_content, matches)) for split in splits]\n",
    "\n",
    "print(f'{len(restored_splits)} splits generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = find_page_breaks(new_md_content)\n",
    "len(breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(splits[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restored_splits[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o')\n",
    "encoding\n",
    "\n",
    "\n",
    "def gen_token_num_plot(splits):\n",
    "    import tiktoken\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "    def get_max_tokens(splits):\n",
    "        max_tokens = 0\n",
    "        for split in splits:\n",
    "            num_tokens = len(encoding.encode(split.page_content))\n",
    "            if num_tokens > max_tokens:\n",
    "                max_tokens = num_tokens\n",
    "        return max_tokens\n",
    "\n",
    "    # get_max_chars(splits)\n",
    "\n",
    "\n",
    "    # Calculate the range for each bar\n",
    "    max_token = get_max_tokens(splits)\n",
    "\n",
    "    # Calculate the count scope for each range\n",
    "    range_size = max_token / 5\n",
    "\n",
    "    # Initialize the count for each range\n",
    "    range_counts = [0] * 5\n",
    "\n",
    "    # Count the number of splits in each range\n",
    "    for split in splits:\n",
    "        num_token = len(encoding.encode(split.page_content))\n",
    "        range_index = min(int(num_token / range_size), 4)\n",
    "        range_counts[range_index] += 1\n",
    "\n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Plot the bar chart\n",
    "    ax.bar(range(1, 6), range_counts)\n",
    "    ax.set_xlabel('Count Scope')\n",
    "    ax.set_ylabel('Number of Splits')\n",
    "    ax.set_title('Number of Splits in Each Range')\n",
    "\n",
    "    # Set the x-axis tick labels to the count scope values\n",
    "    xtick_labels = [f'{int(i * range_size)}-{int((i+1) * range_size)}' for i in range(5)]\n",
    "    ax.set_xticks(range(1, 6))\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(restored_splits)} in total\")\n",
    "gen_token_num_plot(restored_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient, SearchIndexingBufferedSender\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryType,\n",
    "    VectorizedQuery,\n",
    ")\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    SemanticSearch,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswParameters,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "\n",
    "# Configure environment variables\n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "credential = AzureKeyCredential(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"index-multimodal-test\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "\n",
    "try:\n",
    "    result = index_client.delete_index(index_name)\n",
    "    print ('Index', index_name, 'Deleted')\n",
    "except Exception as ex:\n",
    "    print (ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search index\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True,\n",
    "                sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"category\", type=SearchFieldDataType.String,\n",
    "                    filterable=True),\n",
    "    SimpleField(name=\"source\", type=SearchFieldDataType.String,\n",
    "                filterable=True),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW,\n",
    "            parameters=HnswParameters(\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        ),\n",
    "        ExhaustiveKnnAlgorithmConfiguration(\n",
    "            name=\"myExhaustiveKnn\",\n",
    "            kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "            parameters=ExhaustiveKnnParameters(\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        ),\n",
    "        VectorSearchProfile(\n",
    "            name=\"myExhaustiveKnnProfile\",\n",
    "            algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "Read your data, generate OpenAI embeddings and export to a format to insert your Azure AI Search index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(texts, model=model):\n",
    "    return embedding_client.embeddings.create(input=texts, model=model).data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = \"Overview_of_LLMs.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "batch_inputs = [restored_splits[i].page_content for i in range(len(restored_splits))]\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(batch_inputs), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = batch_inputs[batch_start: batch_end]\n",
    "    response = client.embeddings.create(input=batch, model=model)\n",
    "    for i, be in enumerate(response.data):\n",
    "        # double check embeddings are in same order as input\n",
    "        assert i == be.index\n",
    "    batch_embeddings = [e.embedding for e in response.data]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "documents = []\n",
    "counter = 0\n",
    "for item in restored_splits:\n",
    "    file_name = file_prefix.split('.')[0]\n",
    "    documents.append({\n",
    "        'id': str(uuid.uuid4()),\n",
    "        'title':  item.metadata.get('Header 1', file_name),\n",
    "        'content': item.page_content,\n",
    "        'category': item.metadata.get('Header 2', file_name),\n",
    "        'source': file_prefix,\n",
    "        'contentVector': embeddings[counter]\n",
    "    })\n",
    "    counter += 1\n",
    "\n",
    "print(f'{len(documents)} documents generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "admin_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "credential = AzureKeyCredential(admin_key)\n",
    "search_client = SearchClient(endpoint=endpoint,\n",
    "                             index_name=index_name,\n",
    "                             credential=credential)\n",
    "search_client.upload_documents(documents)\n",
    "print(f\"Uploaded {len(documents)} documents in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(query, model=model):\n",
    "    return embedding_client.embeddings.create(input=[query], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Hybrid Search\n",
    "query = \"How is the trend of LLM releases?\"\n",
    "\n",
    "search_client = SearchClient(\n",
    "    service_endpoint, index_name, AzureKeyCredential(key))\n",
    "vector_query = VectorizedQuery(vector=generate_embeddings(query),\n",
    "                               k_nearest_neighbors=5,\n",
    "                               fields=\"contentVector\")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"content\", \"source\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=5\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "output = []\n",
    "for result in results:\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"result: {result}\")\n",
    "    # print(f\"source: {result['source']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "\n",
    "    output.append(result['content'])\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from azure.storage.blob.aio import BlobServiceClient\n",
    "from azure.storage.blob import BlobSasPermissions, generate_blob_sas\n",
    "\n",
    "sas_token_cache = {}\n",
    "\n",
    "\n",
    "def generate_sas_url(blob_client):\n",
    "    # Generate a unique key for the blob\n",
    "    blob_key = f\"{blob_client.account_name}/{blob_client.container_name}/{blob_client.blob_name}\"\n",
    "\n",
    "    # Check if the SAS token is already cached and still valid\n",
    "    if blob_key in sas_token_cache:\n",
    "        sas_token, expiry_time = sas_token_cache[blob_key]\n",
    "        if expiry_time > datetime.now(timezone.utc):\n",
    "            # Return the cached SAS URL if the token is still valid\n",
    "            return f\"https://{blob_client.account_name}.blob.core.windows.net/{blob_client.container_name}/{blob_client.blob_name}?{sas_token}\"\n",
    "\n",
    "    # Generate a new SAS token\n",
    "    expiry_time = datetime.now(timezone.utc) + timedelta(hours=1)\n",
    "    sas_token = generate_blob_sas(\n",
    "        blob_client.account_name,\n",
    "        blob_client.container_name,\n",
    "        blob_client.blob_name,\n",
    "        account_key=blob_client.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=expiry_time\n",
    "    )\n",
    "\n",
    "    # Cache the new SAS token and its expiry time\n",
    "    sas_token_cache[blob_key] = (sas_token, expiry_time)\n",
    "\n",
    "    # Generate the SAS URL\n",
    "    sas_url = f\"https://{blob_client.account_name}.blob.core.windows.net/{blob_client.container_name}/{blob_client.blob_name}?{sas_token}\"\n",
    "\n",
    "    return sas_url\n",
    "\n",
    "\n",
    "def extract_blob_url(input_str):\n",
    "    # Define the regex pattern to match the blob URL\n",
    "    pattern = r'blob://[^\\s\\)]+\\.png'\n",
    "\n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(pattern, input_str)\n",
    "\n",
    "    # Return the list of matched URLs\n",
    "    return matches\n",
    "\n",
    "\n",
    "def extract_elements_from_blob_url(blob_url):\n",
    "    # Split the URL by '/'\n",
    "    elements = blob_url.split('/')\n",
    "\n",
    "    # Extract the required elements\n",
    "    if len(elements) >= 6:\n",
    "        return elements[2], elements[3], elements[4], elements[5]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "async def process(md_content: str):\n",
    "    blob_urls = extract_blob_url(md_content)\n",
    "    sas_urls = {}\n",
    "\n",
    "    async with BlobServiceClient.from_connection_string(os.environ[\"AzureWebJobsStorage\"]) as storage_client:\n",
    "        for blob_url in blob_urls:\n",
    "            elements = extract_elements_from_blob_url(blob_url)\n",
    "            if elements:\n",
    "                _, container_name, blob_name = elements[0], elements[1], '/'.join(\n",
    "                    elements[2:])\n",
    "                blob_client = storage_client.get_blob_client(\n",
    "                    container=container_name, blob=blob_name)\n",
    "                sas_url = generate_sas_url(blob_client)\n",
    "                sas_urls[blob_url] = sas_url\n",
    "\n",
    "    # Replace blob URLs with SAS URLs in the markdown content\n",
    "    for blob_url, sas_url in sas_urls.items():\n",
    "        md_content = md_content.replace(blob_url, sas_url)\n",
    "\n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from IPython.display import Markdown\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryType,\n",
    "    VectorizedQuery,\n",
    ")\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "# Configure environment variables\n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY_GPT_4o_mini\")\n",
    "api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT_GPT_4o_mini\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_MODEl_GPT_4o_mini\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION_GPT_4o_mini\")\n",
    "\n",
    "gpt_client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=api_base,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(query, model=model):\n",
    "    return embedding_client.embeddings.create(input=[query], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "async def infer(query: str, index_name: str):\n",
    "    search_client = SearchClient(\n",
    "        service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    vector_query = VectorizedQuery(vector=generate_embeddings(query),\n",
    "                                   k_nearest_neighbors=5,\n",
    "                                   fields=\"contentVector\")\n",
    "\n",
    "    results = search_client.search(\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"title\", \"content\", \"source\"],\n",
    "        query_type=QueryType.SEMANTIC,\n",
    "        semantic_configuration_name='my-semantic-config',\n",
    "        query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "        query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "        top=5\n",
    "    )\n",
    "\n",
    "    output = []\n",
    "    for result in results:\n",
    "        processed_chunk = await process(result['content'])\n",
    "        output.append(processed_chunk)\n",
    "\n",
    "    context = '\\n'.join(output)\n",
    "\n",
    "    prompt = \"\"\"\n",
    "        {{question}}\n",
    "\n",
    "        Sources:\n",
    "        {{context}}\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt = prompt.replace(\n",
    "        \"{{question}}\", query).replace(\"{{context}}\", context)\n",
    "    \n",
    "    # print(final_prompt)\n",
    "\n",
    "    response = gpt_client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[{'role': 'system', 'content': \"You are a helpful assistant. Try to answer user's question by referencing the following related background information.\\n  If there is not enough information to answer user's question, just say not enough information.\\n  If there are relevant images, please embed the image in the response. DON'T modify the image URL.\"},\n",
    "                  {\"role\": \"user\", \"content\": final_prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is the trend of LLM releases?\"\n",
    "index_name = \"index-multimodal-test\"\n",
    "response = await infer(query, index_name)\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was ERNIE 3.0 released?\"\n",
    "index_name = \"index-multimodal\"\n",
    "response = await infer(query, index_name)\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
